{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jEFAQBwkLRv"
      },
      "source": [
        "# Requirements"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2-xJDyZjpS0"
      },
      "source": [
        "**Requirements:**\n",
        "* `python3` - https://www.python.org/downloads/\n",
        "* Hugging Face Account - https://huggingface.co/join\n",
        "* AWS account - https://aws.amazon.com/resources/create-account/\n",
        "* OpenAI Account - https://platform.openai.com/login?launch *(optional if using ChatOpenAI, which is the default)*\n",
        "\n",
        "You will also probably want an IDE: VS Code is recommended - https://code.visualstudio.com/download\n",
        "\n",
        "Want to get started quickly? Recommended WordPress plugins can be found in the [last section](https://colab.research.google.com/drive/1PGw_QEjJFQ3vhVuSinCbWose5KNjk5wb?authuser=1#scrollTo=a4bfXcyh7p8i&line=1&uniqifier=1 )."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UfY4NYCOXiL"
      },
      "source": [
        "# Code with AI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5YDfSf4uOiqp",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown * Find the repo for Hugging Face Chat UI here - Section 0 allows a no-code deploying in Hugging Face Spaces with a prepopulated template and your choice of LLM: https://github.com/huggingface/chat-ui (Documentation: https://huggingface.co/docs/hub/spaces-sdks-docker-chatui#chatui-on-spaces)\n",
        "#@markdown * `llm-vscode` is an extension you can install on VSCode (https://github.com/huggingface/llm-vscode)\n",
        "#@markdown * It is built upon the StarCoder LLM (https://github.com/bigcode-project/starcoder) - find the playground Starchat here: https://huggingface.co/spaces/HuggingFaceH4/starchat-playground\n",
        "#@markdown * https://sourcegraph.com/cody - Out of the box open source coding assistant that has a free tier and allows you to choose open source models for code completion and chat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DK1Upkh3acn7"
      },
      "source": [
        "# Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-eoO0CKwabkc",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown To use your own data from your WordPress site you're going to start by exporting your `wp_posts` table in your database as a CSV. The easiest method is to use PHPMyAdmin (https://docs.bitnami.com/aws/apps/dolibarr/administration/export-database/ - ensure you are exporting as CSV and not SQL as indicated in the documentation).\n",
        "#@markdown If you don't have sufficient data (say at least 50-100 objects), you may choose to supplement. A good opensource directory for datasets can be found on https://www.kaggle.com/datasets/\n",
        "\n",
        "#@markdown You will need to decide on the foundational model you will be working with at this point as it may dictate the data structure you will need to follow.\n",
        "\n",
        "# @markdown In this example, we're going to be using Tiny Llama and quantized version of Llama2 (HuggingFace Repo: TinyLlama/TinyLlama-1.1B-Chat-v1.0)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dq6Hul2HOGPF"
      },
      "source": [
        "# Finetune your chosen foundational LLM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_tOjDZxqdzP"
      },
      "source": [
        "***NOTE: There is a 90min idle timeout - if finetuning is going to take longer than 90 minutes (it will be in the output below) make sure you remember to interact with this notebook - there is currently no known programmatic way to defeat this.***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gi1Mxr6CMw3C",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title ðŸ¤— AutoTrain Advanced\n",
        "#@markdown In order to use this colab\n",
        "#@markdown - upload train.csv to a folder named `data/`\n",
        "#@markdown - train.csv must contain a `text` column\n",
        "#@markdown - choose a project name if you wish\n",
        "#@markdown - change model if you wish, you can use most of the text-generation models from Hugging Face Hub\n",
        "#@markdown - add huggingface information (token and repo_id) if you wish to push trained model to huggingface hub - you do not need to create the repo in advance.\n",
        "#@markdown - update hyperparameters if you wish, this is not necessary (hyperparameters matter very little).\n",
        "#@markdown - click `Runtime > Run all` or run each cell individually\n",
        "#@markdown - This code comes from the Hugging Face Autotrain-Advanced Repo: https://github.com/huggingface/autotrain-advanced/issues\n",
        "\n",
        "import os\n",
        "!pip install -U autotrain-advanced > install_logs.txt\n",
        "!autotrain setup --colab > setup_logs.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-giaOz9N9lK"
      },
      "outputs": [],
      "source": [
        "#@markdown ---\n",
        "#@markdown #### Project Config\n",
        "#@markdown Note: if you are using a restricted/private model, you need to enter your Hugging Face token in the next step.\n",
        "project_name = 'example-project' # @param {type:\"string\"}\n",
        "model_name = 'TinyLlama/TinyLlama-1.1B-Chat-v1.0' # @param {type:\"string\"}\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown #### Push to Hub?\n",
        "#@markdown Use these only if you want to push your trained model to a private repo in your Hugging Face Account\n",
        "#@markdown If you dont use these, the model will be saved in Google Colab and you are required to download it manually.\n",
        "#@markdown Please enter your Hugging Face write token. The trained model will be saved to your Hugging Face account.\n",
        "#@markdown You can find your token here: https://huggingface.co/settings/tokens\n",
        "push_to_hub = True # @param [\"False\", \"True\"] {type:\"raw\"}\n",
        "hf_token = \"hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\" #@param {type:\"string\"}\n",
        "repo_id = \"huggingface_username/repo_name\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown #### Hyperparameters\n",
        "learning_rate = 2e-4 # @param {type:\"number\"}\n",
        "num_epochs = 1 #@param {type:\"number\"}\n",
        "batch_size = 1 # @param {type:\"slider\", min:1, max:32, step:1}\n",
        "block_size = 512 # @param {type:\"number\"}\n",
        "trainer = \"sft\" # @param [\"default\", \"sft\"] {type:\"raw\"}\n",
        "warmup_ratio = 0.1 # @param {type:\"number\"}\n",
        "weight_decay = 0.01 # @param {type:\"number\"}\n",
        "gradient_accumulation = 4 # @param {type:\"number\"}\n",
        "mixed_precision = \"fp16\" # @param [\"fp16\", \"bf16\", \"none\"] {type:\"raw\"}\n",
        "peft = True # @param [\"False\", \"True\"] {type:\"raw\"}\n",
        "quantization = \"int4\" # @param [\"int4\", \"int8\", \"none\"] {type:\"raw\"}\n",
        "lora_r = 16 #@param {type:\"number\"}\n",
        "lora_alpha = 32 #@param {type:\"number\"}\n",
        "lora_dropout = 0.05 #@param {type:\"number\"}\n",
        "is_decoder = True\n",
        "\n",
        "os.environ[\"PROJECT_NAME\"] = project_name\n",
        "os.environ[\"MODEL_NAME\"] = model_name\n",
        "os.environ[\"PUSH_TO_HUB\"] = str(push_to_hub)\n",
        "os.environ[\"HF_TOKEN\"] = hf_token\n",
        "os.environ[\"REPO_ID\"] = repo_id\n",
        "os.environ[\"LEARNING_RATE\"] = str(learning_rate)\n",
        "os.environ[\"NUM_EPOCHS\"] = str(num_epochs)\n",
        "os.environ[\"BATCH_SIZE\"] = str(batch_size)\n",
        "os.environ[\"BLOCK_SIZE\"] = str(block_size)\n",
        "os.environ[\"WARMUP_RATIO\"] = str(warmup_ratio)\n",
        "os.environ[\"WEIGHT_DECAY\"] = str(weight_decay)\n",
        "os.environ[\"GRADIENT_ACCUMULATION\"] = str(gradient_accumulation)\n",
        "os.environ[\"MIXED_PRECISION\"] = str(mixed_precision)\n",
        "os.environ[\"PEFT\"] = str(peft)\n",
        "os.environ[\"QUANTIZATION\"] = str(quantization)\n",
        "os.environ[\"LORA_R\"] = str(lora_r)\n",
        "os.environ[\"LORA_ALPHA\"] = str(lora_alpha)\n",
        "os.environ[\"LORA_DROPOUT\"] = str(lora_dropout)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hUQKm9L0OABn"
      },
      "outputs": [],
      "source": [
        "!autotrain llm \\\n",
        "--train \\\n",
        "--model ${MODEL_NAME} \\\n",
        "--project-name ${PROJECT_NAME} \\\n",
        "--data-path data/ \\\n",
        "--text-column text \\\n",
        "--lr ${LEARNING_RATE} \\\n",
        "--batch-size ${BATCH_SIZE} \\\n",
        "--epochs ${NUM_EPOCHS} \\\n",
        "--block-size ${BLOCK_SIZE} \\\n",
        "--warmup-ratio ${WARMUP_RATIO} \\\n",
        "--lora-r ${LORA_R} \\\n",
        "--lora-alpha ${LORA_ALPHA} \\\n",
        "--lora-dropout ${LORA_DROPOUT} \\\n",
        "--weight-decay ${WEIGHT_DECAY} \\\n",
        "--gradient-accumulation ${GRADIENT_ACCUMULATION} \\\n",
        "--quantization ${QUANTIZATION} \\\n",
        "--mixed-precision ${MIXED_PRECISION} \\\n",
        "$( [[ \"$PEFT\" == \"True\" ]] && echo \"--peft\" ) \\\n",
        "$( [[ \"$PUSH_TO_HUB\" == \"True\" ]] && echo \"--push-to-hub --token ${HF_TOKEN} --repo-id ${REPO_ID}\" )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0vsSzbWQL8X"
      },
      "source": [
        "# Deploy ChromaDB Vector DB to AWS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfLFFEVuSLpO"
      },
      "source": [
        "Create your AWS CloudFormation (https://aws.amazon.com/cloudformation/) stack for ChromaDB Vector Database with the following JSON template - user is default `ec2-user` (copy, paste and save as a JSON file then use the \"Upload a template file\" option when creating a new stack):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XPBspQeEQRkv"
      },
      "outputs": [],
      "source": [
        "{\n",
        "  \"AWSTemplateFormatVersion\": \"2010-09-09\",\n",
        "  \"Description\": \"Create a stack that runs Chroma hosted on a single dockerized instance with API key credentials\",\n",
        "  \"Parameters\": {\n",
        "    \"KeyName\": {\n",
        "      \"Description\": \"Name of an existing EC2 KeyPair to enable SSH access to the instance\",\n",
        "      \"Type\": \"String\",\n",
        "      \"ConstraintDescription\": \"If present, must be the name of an existing EC2 KeyPair.\",\n",
        "      \"Default\": \"\"\n",
        "    },\n",
        "    \"InstanceType\": {\n",
        "      \"Description\": \"EC2 instance type (t3.small minimum)\",\n",
        "      \"Type\": \"String\",\n",
        "      \"Default\": \"t3.small\"\n",
        "    },\n",
        "    \"ChromaApiToken\": {\n",
        "      \"Description\": \"API token used to connect with X-Chroma-Token header during connection\",\n",
        "      \"Type\": \"String\",\n",
        "      \"Default\": \"insert-your-unique-token-here\"\n",
        "    }\n",
        "  },\n",
        "  \"Conditions\": {\n",
        "    \"HasKeyName\": {\n",
        "      \"Fn::Not\": [\n",
        "        {\n",
        "          \"Fn::Equals\": [\n",
        "            {\n",
        "              \"Ref\": \"KeyName\"\n",
        "            },\n",
        "            \"\"\n",
        "          ]\n",
        "        }\n",
        "      ]\n",
        "    }\n",
        "  },\n",
        "  \"Resources\": {\n",
        "    \"ChromaInstance\": {\n",
        "      \"Type\": \"AWS::EC2::Instance\",\n",
        "      \"Properties\": {\n",
        "        \"ImageId\": {\n",
        "          \"Fn::FindInMap\": [\n",
        "            \"Region2AMI\",\n",
        "            {\n",
        "              \"Ref\": \"AWS::Region\"\n",
        "            },\n",
        "            \"AMI\"\n",
        "          ]\n",
        "        },\n",
        "        \"InstanceType\": {\n",
        "          \"Ref\": \"InstanceType\"\n",
        "        },\n",
        "        \"UserData\": {\n",
        "          \"Fn::Base64\": {\n",
        "            \"Fn::Join\": [\n",
        "              \"\",\n",
        "              [\n",
        "                \"Content-Type: multipart/mixed; boundary=\\\"//\\\"\\n\",\n",
        "                \"MIME-Version: 1.0\\n\",\n",
        "                \"\\n\",\n",
        "                \"--//\\n\",\n",
        "                \"Content-Type: text/cloud-config; charset=\\\"us-ascii\\\"\\n\",\n",
        "                \"MIME-Version: 1.0\\n\",\n",
        "                \"Content-Transfer-Encoding: 7bit\\n\",\n",
        "                \"Content-Disposition: attachment; filename=\\\"cloud-config.txt\\\"\\n\",\n",
        "                \"\\n\",\n",
        "                \"\\n\",\n",
        "                \"#cloud-config\\n\",\n",
        "                \"cloud_final_modules:\\n\",\n",
        "                \"- [scripts-user, always]\\n\",\n",
        "                \"\\n\",\n",
        "                \"\\n\",\n",
        "                \"--//\\n\",\n",
        "                \"Content-Type: text/x-shellscript; charset=\\\"us-ascii\\\"\\n\",\n",
        "                \"MIME-Version: 1.0\\n\",\n",
        "                \"Content-Transfer-Encoding: 7bit\\n\",\n",
        "                \"Content-Disposition: attachment; filename=\\\"userdata.txt\\\"\\n\",\n",
        "                \"\\n\",\n",
        "                \"\\n\",\n",
        "                \"#!/bin/bash\\n\",\n",
        "                \"# check output of userdata script with sudo tail -f /var/log/cloud-init-output.log\\n\",\n",
        "                \"yum install docker sqlite tree git -y\\n\",\n",
        "                \"usermod -a -G docker ec2-user\\n\",\n",
        "                \"systemctl enable docker\\n\",\n",
        "                \"systemctl start docker\\n\",\n",
        "                \"mkdir /home/ec2-user/chroma-storage\\n\",\n",
        "                \"git clone https://gist.github.com/dr-robert-li/c0524724a75954e44adf1be546fbfd2c /home/ec2-user/run-script/\\n\",\n",
        "                \"mv /home/ec2-user/run-script/run.sh /home/ec2-user/run.sh\\n\",\n",
        "                \"rm -rf /home/ec2-user/run-script/\\n\",\n",
        "                \"chown ec2-user:ec2-user /home/ec2-user/chroma-storage\\n\",\n",
        "                \"chmod +x /home/ec2-user/run.sh\\n\",\n",
        "                \"docker pull chromadb/chroma\\n\",\n",
        "                \"docker run -d -p 8000:8000 -e CHROMA_SERVER_AUTH_CREDENTIALS_PROVIDER=\\\"chromadb.auth.token.TokenConfigServerAuthCredentialsProvider\\\" -e CHROMA_SERVER_AUTH_PROVIDER=\\\"chromadb.auth.token.TokenAuthServerProvider\\\"\",\n",
        "                {\n",
        "                  \"Fn::Sub\": \" -e CHROMA_SERVER_AUTH_CREDENTIALS=\\\"${ChromaApiToken}\\\"\"\n",
        "                },\n",
        "                \" -e CHROMA_SERVER_AUTH_TOKEN_TRANSPORT_HEADER=\\\"X_CHROMA_TOKEN\\\" -v /home/ec2-user/chroma-storage/:/chroma/chroma/ chromadb/chroma\\n\",\n",
        "                \"\\n\",\n",
        "                \"--//--\\n\"\n",
        "              ]\n",
        "            ]\n",
        "          }\n",
        "        },\n",
        "        \"SecurityGroupIds\": [\n",
        "          {\n",
        "            \"Ref\": \"ChromaInstanceSecurityGroup\"\n",
        "          }\n",
        "        ],\n",
        "        \"KeyName\": {\n",
        "          \"Fn::If\": [\n",
        "            \"HasKeyName\",\n",
        "            {\n",
        "              \"Ref\": \"KeyName\"\n",
        "            },\n",
        "            {\n",
        "              \"Ref\": \"AWS::NoValue\"\n",
        "            }\n",
        "          ]\n",
        "        },\n",
        "        \"BlockDeviceMappings\": [\n",
        "          {\n",
        "            \"DeviceName\": {\n",
        "              \"Fn::FindInMap\": [\n",
        "                \"Region2AMI\",\n",
        "                {\n",
        "                  \"Ref\": \"AWS::Region\"\n",
        "                },\n",
        "                \"RootDeviceName\"\n",
        "              ]\n",
        "            },\n",
        "            \"Ebs\": {\n",
        "              \"VolumeSize\": 24\n",
        "            }\n",
        "          }\n",
        "        ]\n",
        "      }\n",
        "    },\n",
        "    \"ChromaInstanceSecurityGroup\": {\n",
        "      \"Type\": \"AWS::EC2::SecurityGroup\",\n",
        "      \"Properties\": {\n",
        "        \"GroupDescription\": \"Chroma Instance Security Group\",\n",
        "        \"SecurityGroupIngress\": [\n",
        "          {\n",
        "            \"IpProtocol\": \"tcp\",\n",
        "            \"FromPort\": \"22\",\n",
        "            \"ToPort\": \"22\",\n",
        "            \"CidrIp\": \"0.0.0.0/0\"\n",
        "          },\n",
        "          {\n",
        "            \"IpProtocol\": \"tcp\",\n",
        "            \"FromPort\": \"8000\",\n",
        "            \"ToPort\": \"8000\",\n",
        "            \"CidrIp\": \"0.0.0.0/0\"\n",
        "          }\n",
        "        ]\n",
        "      }\n",
        "    }\n",
        "  },\n",
        "  \"Outputs\": {\n",
        "    \"ServerIp\": {\n",
        "      \"Description\": \"IP address of the Chroma server\",\n",
        "      \"Value\": {\n",
        "        \"Fn::GetAtt\": [\n",
        "          \"ChromaInstance\",\n",
        "          \"PublicIp\"\n",
        "        ]\n",
        "      }\n",
        "    }\n",
        "  },\n",
        "  \"Mappings\": {\n",
        "    \"Region2AMI\": {\n",
        "      \"ap-south-1\": {\n",
        "        \"AMI\": \"ami-03cb1380eec7cc118\",\n",
        "        \"RootDeviceName\": \"/dev/xvda\"\n",
        "      },\n",
        "      \"eu-north-1\": {\n",
        "        \"AMI\": \"ami-078e13ebe3b027f1c\",\n",
        "        \"RootDeviceName\": \"/dev/xvda\"\n",
        "      },\n",
        "      \"eu-west-3\": {\n",
        "        \"AMI\": \"ami-00575c0cbc20caf50\",\n",
        "        \"RootDeviceName\": \"/dev/xvda\"\n",
        "      },\n",
        "      \"eu-west-2\": {\n",
        "        \"AMI\": \"ami-0b026d11830afcbac\",\n",
        "        \"RootDeviceName\": \"/dev/xvda\"\n",
        "      },\n",
        "      \"eu-west-1\": {\n",
        "        \"AMI\": \"ami-06e0ce9d3339cb039\",\n",
        "        \"RootDeviceName\": \"/dev/xvda\"\n",
        "      },\n",
        "      \"ap-northeast-3\": {\n",
        "        \"AMI\": \"ami-0171e161a6e0c595c\",\n",
        "        \"RootDeviceName\": \"/dev/xvda\"\n",
        "      },\n",
        "      \"ap-northeast-2\": {\n",
        "        \"AMI\": \"ami-0eb14fe5735c13eb5\",\n",
        "        \"RootDeviceName\": \"/dev/xvda\"\n",
        "      },\n",
        "      \"ap-northeast-1\": {\n",
        "        \"AMI\": \"ami-08a8688fb7eacb171\",\n",
        "        \"RootDeviceName\": \"/dev/xvda\"\n",
        "      },\n",
        "      \"ca-central-1\": {\n",
        "        \"AMI\": \"ami-0843f7c45354d48b5\",\n",
        "        \"RootDeviceName\": \"/dev/xvda\"\n",
        "      },\n",
        "      \"sa-east-1\": {\n",
        "        \"AMI\": \"ami-0344e5787e2e93144\",\n",
        "        \"RootDeviceName\": \"/dev/xvda\"\n",
        "      },\n",
        "      \"ap-southeast-1\": {\n",
        "        \"AMI\": \"ami-0753e0e42b20e96e3\",\n",
        "        \"RootDeviceName\": \"/dev/xvda\"\n",
        "      },\n",
        "      \"ap-southeast-2\": {\n",
        "        \"AMI\": \"ami-047dcdc46ac4f2e6b\",\n",
        "        \"RootDeviceName\": \"/dev/xvda\"\n",
        "      },\n",
        "      \"eu-central-1\": {\n",
        "        \"AMI\": \"ami-004359656ecac6a95\",\n",
        "        \"RootDeviceName\": \"/dev/xvda\"\n",
        "      },\n",
        "      \"us-east-1\": {\n",
        "        \"AMI\": \"ami-0ed9277fb7eb570c9\",\n",
        "        \"RootDeviceName\": \"/dev/xvda\"\n",
        "      },\n",
        "      \"us-east-2\": {\n",
        "        \"AMI\": \"ami-064ff912f78e3e561\",\n",
        "        \"RootDeviceName\": \"/dev/xvda\"\n",
        "      },\n",
        "      \"us-west-1\": {\n",
        "        \"AMI\": \"ami-0746394790be7162e\",\n",
        "        \"RootDeviceName\": \"/dev/xvda\"\n",
        "      },\n",
        "      \"us-west-2\": {\n",
        "        \"AMI\": \"ami-098e42ae54c764c35\",\n",
        "        \"RootDeviceName\": \"/dev/xvda\"\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0c2TlNulV2i"
      },
      "source": [
        "If you would like to restart and reset the ChromaDB API Key and/or the ChromaDB storage location simply invoke the `run.sh` script in the root `/home/ec2-user/` folder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsJySnyFvZof"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzcFnhaFDntN"
      },
      "source": [
        "\n",
        "# Embed Data for RAG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQU0U3oXpFjc"
      },
      "source": [
        "The following steps will allow you to take your data in `csv` format, tokenize it and embed it into the previously created ChromaDB Vector database using open source models where possible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2OgPFyh1aI9"
      },
      "source": [
        "Start by installing dependencies:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MPqUzC8-mVrp"
      },
      "outputs": [],
      "source": [
        "pip install pandas langchain chromadb requests langchain_community openai tiktoken huggingface_hub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRuFLEuzpWKP"
      },
      "source": [
        "Make sure to modify the parameters in the Python scripts below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z29GjY9b1iNi"
      },
      "outputs": [],
      "source": [
        "# Importing Modules and Dependencies\n",
        "import os\n",
        "import pandas as pd\n",
        "import chromadb\n",
        "import tiktoken\n",
        "# tiktoken is required for generating OpenAI Embeddings and OpenAI powered text splitting\n",
        "\n",
        "from langchain.schema import Document\n",
        "from langchain.document_loaders import DataFrameLoader\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.llms import HuggingFaceHub\n",
        "from langchain.llms import HuggingFaceEndpoint\n",
        "from langchain.embeddings import HuggingFaceHubEmbeddings\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "# @markdown Enter your API Keys\n",
        "\n",
        "# @markdown When using HuggingFace Hub make sure to use the WRITE API Token\n",
        "hugging_face_key = \"hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\" # @param {type:\"string\"}\n",
        "\n",
        "# @markdown Unset the below if you don't want to use OpenAI/ChatOpenAI (currently Llama2 Chat is set behind an approval process) - ChatOpenAI is the default.\n",
        "openai_key = \"sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\" # @param {type:\"string\"}\n",
        "\n",
        "# Set env API Key\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = hugging_face_key\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai_key"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown Upload your cleaned data csv and indicate the location of the file below.\n",
        "\n",
        "# Creating pandas dataframe from file\n",
        "data_file = '/path/to/data/file.csv' # @param {type:\"string\"}\n",
        "df = pd.read_csv(data_file)\n",
        "\n",
        "# Doublecheck your data\n",
        "df.head()"
      ],
      "metadata": {
        "id": "6LwijM4k1vHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "28EleitW1oae"
      },
      "outputs": [],
      "source": [
        "# Formatting data using langchain loader\n",
        "loader = DataFrameLoader(df, page_content_column=\"text\")\n",
        "# Set page_content_column to the main text of the document - usually you should name this column \"text\" (case sensitive) for interoperability\n",
        "\n",
        "# Defining document data\n",
        "data = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lwUQ7PCM23IG"
      },
      "outputs": [],
      "source": [
        "# Defining Embedding Function\n",
        "model = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "embeddings = HuggingFaceHubEmbeddings(\n",
        "                model=model,\n",
        "            )\n",
        "# embeddings = OpenAIEmbeddings() # Set if you want to use OpenAI comment out the \"model\" and \"embeddings\" variables above"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lqq3It8i25uP"
      },
      "outputs": [],
      "source": [
        "# @markdown Define your ChromaDB client/server settings (also can be used to test along with the commands below):\n",
        "chroma_hostip = '0.0.0.0' # @param {type:\"string\"}\n",
        "chroma_hostport = '8000' # @param {type:\"string\"}\n",
        "chroma_api_token = '0123456789' # @param {type:\"string\"}\n",
        "\n",
        "client = chromadb.HttpClient(\n",
        "    host=chroma_hostip,\n",
        "    port=chroma_hostport,\n",
        "    headers={'X-Chroma-Token': chroma_api_token}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iiEw1nvUKbqZ"
      },
      "outputs": [],
      "source": [
        "# @markdown Define your ChromaDB Collection Name - it will create one if it does not exist.\n",
        "vector_collection_name = 'name-of-collection' # @param {type:\"string\"}\n",
        "\n",
        "# Creating (if not found) collection\n",
        "# collection = client.get_or_create_collection(name=vector_collection_name)\n",
        "collection = client.get_collection(name=vector_collection_name) # Test query to ChromaDB instance"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the VectorDB store settings for getting data, embedding, we're also telling the script to send 'data' to ChromaDB client (refer above) and collection_name\n",
        "# vectordb = Chroma.from_documents(documents=data, embedding=embeddings, client=client, collection_name=vector_collection_name) # Use this to embed data\n",
        "vectordb = Chroma(embedding_function=embeddings, client=client, collection_name=vector_collection_name) # Use this to reference existing embeddings"
      ],
      "metadata": {
        "id": "g1UPlAEkQl0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dy3I5oq729ju"
      },
      "outputs": [],
      "source": [
        "# List all ChromaDB collections on ChromaDB server to check\n",
        "client.list_collections()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ieY_-5wAJ9Nm"
      },
      "outputs": [],
      "source": [
        "# Use this command if you with to delete a found collection\n",
        "# client.delete_collection(name=\"collection-name\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oRe1uQb_296Y"
      },
      "outputs": [],
      "source": [
        "# Validate the data has been stored on the ChromaDB instance in the right collection\n",
        "collection.count() # Use this command to check the number of items in the collection\n",
        "# collection.peek() # Use this command if you want to check the first 10 items in the collection"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use this command if you want to delete items in the substantiated collection\n",
        "# collection.delete(\n",
        "#     ids = [ 'id1', 'id2', 'id3' ]\n",
        "#     )\n",
        "# collection.modify(name=\"insert-new-name\") # Use this command to modify the name of the substantiated collection."
      ],
      "metadata": {
        "id": "yVnUBJFlauri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nKOKpKb82_4O"
      },
      "outputs": [],
      "source": [
        "# Defining document retriever to provide context to eventual QA\n",
        "# k = the max number of results to return, delete if no limit is required;\n",
        "# search_type defines the type of search, similarity (default) or mmr are accepted\n",
        "# (MMR refers to maximal marginal relevance - iteratively searching for dissimilar documents: https://docs.llamaindex.ai/en/latest/examples/vector_stores/SimpleIndexDemoMMR.html)\n",
        "# retriever = vectordb.as_retriever(search_kwargs={\"k\": 10}, search_type=\"similarity\") # Example with settings\n",
        "retriever = vectordb.as_retriever()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UvjRF0Dw3BRE"
      },
      "outputs": [],
      "source": [
        "# @markdown Test that the retriever with a prompt:\n",
        "retriever_prompt = 'What is the most thing in the collection?' # @param {type:\"string\"}\n",
        "retrieved_docs = retriever.invoke(retriever_prompt)\n",
        "print(retrieved_docs[0].page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mP2Ach8O3XNN"
      },
      "source": [
        "Once data has been embedded, let's create a Retrieval Augmented Generation (RAG) Chain to ask a question with context.\n",
        "\n",
        "You can modify the `template = \"\"\" \"\"\"` below to change the context and test the RAG Chain to see what output you get.\n",
        "\n",
        "You can also modify which LLM model to use.\n",
        "\n",
        "There are 2 options below. OpenAI's GPT models are easier to use and have much higher max token limitations (16k for GPT 3.5 and up to 32k for GPT4 as of Jan 2024) compared to open source models, but if you wish to use an open source model, the second cell below allows for that.\n",
        "\n",
        "Keep in mind that because it is using a summarization chain, ***prompt detail will be lost.*** The output will attempt to compensate using the wider LLM dataset.\n",
        "\n",
        "You can choose to use an Open Source LLM that has 16k or 32k context limits such as `TheBloke/Llama-2-13B-chat-GPTQ` or `NurtureAI/OpenHermes-2.5-Mistral-7B-16k` which would allow a higher `max_token_limit` parameter and more iterations. Keep in mind this will require a large amount of memory in your GPU instance.\n",
        "\n",
        "***You cannot use both at the same time.***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kLwTmrHK3bN3"
      },
      "outputs": [],
      "source": [
        "# @markdown ###Option 1: ChatOpenAI OpenAI model\n",
        "\n",
        "# @markdown No additional input required.\n",
        "# Define LLM Model to use for later QA text generation\n",
        "llm = ChatOpenAI() # Set if you want to use ChatOpenAI\n",
        "\n",
        "template = \"\"\"\n",
        "Answer the question based only on the following context:\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template) # set if using ChatOpenAI Model\n",
        "\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join([d.page_content for d in docs])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown ###Option 2: Open Source LLM\n",
        "\n",
        "# @markdown This will use a Hugging Face Hosted Model e.g. `mistralai/Mistral-7B-Instruct-v0.1`\n",
        "\n",
        "# @markdown This will require a hosted Inference Endpoint (https://ui.endpoints.huggingface.co/).\n",
        "# @markdown There may be max token limitations to the model (multiply `num_interations` by `model_token_limit` to determine total number of tokens being used) and each model's inference endpoint differs in usage. Make sure to read the instructions.\n",
        "# @markdown Below is only a suggested workflow. Make sure to modify the prompting structure.\n",
        "\n",
        "# @markdown Make sure to retain the (\"[URI]\") structure for the Hugging Face Endpoint.\n",
        "\n",
        "# Define LLM Model to use for later QA text generation\n",
        "endpoint_url = \"(\\\"https://xxxxxxxxxxxxxxxx.us-east-1.aws.endpoints.huggingface.cloud\\\")\" # @param {type:\"string\"}\n",
        "llm = HuggingFaceEndpoint(\n",
        "    endpoint_url=endpoint_url,\n",
        "    huggingfacehub_api_token=hugging_face_key,\n",
        "    task='text-generation'\n",
        ")\n",
        "\n",
        "model_token_limit = 1024 # @param {type:\"number\"}\n",
        "num_iterations = 2 # @param {type:\"number\"}\n",
        "\n",
        "# @markdown The included prompting template is designed to work with the `mistralai/Mistral` family of LLMs.\n",
        "template = \"\"\"\n",
        "<s>\n",
        "\"[INST] You are an expert baker always ready to provide tips, tricks and recipes as well as tasting notes![/INST]\"\n",
        "\"[INST] Based on the following Question: {question} - Can you a summary of the context provided: {context}?[/INST]\"\n",
        "</s>\n",
        "\"\"\"\n",
        "\n",
        "# Text Splitter and Summary Chain to ensure prompts don't exceed token limit of model, make sure to modify chunk_size to reflect max tokens of model\n",
        "# (very important for open source models with limited max token limits)\n",
        "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=model_token_limit, chunk_overlap=100\n",
        ")\n",
        "template_split = text_splitter.split_text(template)\n",
        "\n",
        "# Formatting template and loading into dataframe.\n",
        "template_df = pd.DataFrame(\n",
        "    {\"text\": template_split}\n",
        ")\n",
        "template_loader = DataFrameLoader(template_df, page_content_column=\"text\")\n",
        "template_data = template_loader.load()\n",
        "\n",
        "# Summarizing chain. Choose from refine or map_reduce. Turn verbose to True if you wish to see what it's doing.\n",
        "# chain = load_summarize_chain(llm, chain_type=\"refine\", verbose=False)\n",
        "chain = load_summarize_chain(llm, chain_type=\"map_reduce\", verbose=False)\n",
        "\n",
        "# Invoke summarizing chain and load into template\n",
        "template_summ = chain.run(template_data[:num_iterations])\n",
        "\n",
        "template_final = \"\"\"\n",
        "You are an expert baker always ready to provide tips, tricks and recipes as well as tasting notes!\n",
        "Answer the question based only on the following context: \"\"\" + template_summ + \"\"\"\n",
        "Question: {question}\n",
        "If the context isn't sufficient answer as best as possible ignoring context.\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template_final)\n",
        "\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join([d.page_content for d in docs])\n"
      ],
      "metadata": {
        "id": "ePrJtA19oshc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jBdfZRpp3fG6"
      },
      "outputs": [],
      "source": [
        "# This is the full chain with commands piped into each other.\n",
        "# We're reusing the retriever variable set above.\n",
        "# The llm used will depend on the template cell chosen above.\n",
        "\n",
        "QAchain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# @markdown Test the RAG Chain to see that it worked\n",
        "\n",
        "chain_prompt = \"Give me some information that can be inferred from the dataset but isn't actually in it.\" # @param {type:\"string\"}\n",
        "QAchain.invoke(chain_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBo8sF5E4401"
      },
      "source": [
        "\n",
        "# Use LangServe to Create FastAPI Endpoints\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gLqDZgtp0Ju"
      },
      "source": [
        "To use the RAG Chain in a production setting, you will need to deploy it on the cloud.\n",
        "\n",
        "Start by using LangServe (using FastAPI) to create the API endpoints that will allow external applications to interface with it. The LangServe application will retrieve embeddings from the ChromaDB Vector DB (from above) and run them through a similar RAG chain as above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQSKf4XHOLg6"
      },
      "source": [
        "Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0qxm3cybN2Rr"
      },
      "outputs": [],
      "source": [
        "pip install langchain openai tiktoken chromadb langserve sse_starlette fastapi[all] uvicorn mangum"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7Upf6iGOP2r"
      },
      "source": [
        "Create a `main.py` file (for easier deployment later) with the following code and run it on your server host environment. Default is `localhost:8000`\n",
        "\n",
        "This comes from the LangServe Examples repo where there are many other examples and templates you can start from: https://github.com/langchain-ai/langserve/tree/main/examples/conversational_retrieval_chain\n",
        "\n",
        "As per the above, there are 2 python scripts below. Option 1 is to use OpenAI for ease of use. Option 2 is to use an open source LLM via HuggingFace Endpoints (https://ui.endpoints.huggingface.co/) - ***Please make sure to read the text around choosing an appropriate model to deploy for Option 2***.\n",
        "\n",
        "Again, ***you cannot use both.***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plWS8VO9NWSX"
      },
      "outputs": [],
      "source": [
        "# @markdown ###Option 1 - OpenAI\n",
        "\n",
        "from operator import itemgetter\n",
        "from typing import List, Tuple\n",
        "import chromadb\n",
        "import os\n",
        "import langserve\n",
        "import mangum\n",
        "\n",
        "from fastapi import FastAPI\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.embeddings import HuggingFaceHubEmbeddings\n",
        "from langchain.llms import HuggingFaceEndpoint\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.prompts.prompt import PromptTemplate\n",
        "from langchain.schema import format_document\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain.schema.runnable import RunnableMap, RunnablePassthrough\n",
        "from langchain.vectorstores import Chroma\n",
        "from langserve import RemoteRunnable\n",
        "\n",
        "from langserve import add_routes\n",
        "from langserve.pydantic_v1 import BaseModel, Field\n",
        "\n",
        "# @markdown Enter your API Keys\n",
        "\n",
        "# @markdown When using HuggingFace Hub make sure to use the WRITE API Token\n",
        "hugging_face_key = \"hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\" # @param {type:\"string\"}\n",
        "\n",
        "# @markdown Unset the below if you don't want to use OpenAI/ChatOpenAI (currently Llama2 Chat is set behind an approval process) - ChatOpenAI is the default.\n",
        "openai_key = \"sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\" # @param {type:\"string\"}\n",
        "\n",
        "# Set env API Key\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = hugging_face_key\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai_key\n",
        "\n",
        "# Create a template where chat history provides additional dynamic context for next answer\n",
        "_TEMPLATE = \"\"\"Given the following conversation and a follow up question, rephrase the\n",
        "follow up question to be a standalone question, in its original language.\n",
        "\n",
        "Chat History:\n",
        "{chat_history}\n",
        "Follow Up Input: {question}\n",
        "Standalone question:\"\"\"\n",
        "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_TEMPLATE)\n",
        "\n",
        "# Create a standard Q&A template to provide additional persistent context for the next answer\n",
        "ANSWER_TEMPLATE = \"\"\"You are a world-class baker and pastry chef.\n",
        "I will ask you a question and you will provide an answer based on the most relevant recipe you know, as well as commentary on best practices, tips, and any related baked goods that should be considered.\n",
        "\n",
        "You will follow ALL the rules below:\n",
        "\n",
        "1. You will use recipes that are in the database.\n",
        "2. If you cannot find a relevant recipe in the database you will come up with a relevant receipe in the same style as the recipes in the database.\n",
        "\n",
        "Here is the most relevant recipe in the database:\n",
        "{context}\n",
        "\n",
        "This is the question you are being asked: {question}\n",
        "\n",
        "Please answer with the above context.\n",
        "\"\"\"\n",
        "ANSWER_PROMPT = ChatPromptTemplate.from_template(ANSWER_TEMPLATE)\n",
        "\n",
        "DEFAULT_DOCUMENT_PROMPT = PromptTemplate.from_template(template=\"{page_content}\")\n",
        "\n",
        "# Formatting functions\n",
        "def _combine_documents(\n",
        "    docs, document_prompt=DEFAULT_DOCUMENT_PROMPT, document_separator=\"\\n\\n\"\n",
        "):\n",
        "    \"\"\"Combine documents into a single string.\"\"\"\n",
        "    doc_strings = [format_document(doc, document_prompt) for doc in docs]\n",
        "    return document_separator.join(doc_strings)\n",
        "\n",
        "def _format_chat_history(chat_history: List[Tuple]) -> str:\n",
        "    \"\"\"Format chat history into a string.\"\"\"\n",
        "    buffer = \"\"\n",
        "    for dialogue_turn in chat_history:\n",
        "        human = \"Human: \" + dialogue_turn[0]\n",
        "        ai = \"Assistant: \" + dialogue_turn[1]\n",
        "        buffer += \"\\n\" + \"\\n\".join([human, ai])\n",
        "    return buffer\n",
        "\n",
        "# Defining Embedding Function\n",
        "model = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "embeddings = HuggingFaceHubEmbeddings(\n",
        "                model=model,\n",
        "            )\n",
        "\n",
        "# @markdown Define your ChromaDB client/server settings\n",
        "chroma_hostip = '0.0.0.0' # @param {type:\"string\"}\n",
        "chroma_hostport = '8000' # @param {type:\"string\"}\n",
        "chroma_api_token = '0123456789' # @param {type:\"string\"}\n",
        "\n",
        "client = chromadb.HttpClient(\n",
        "    host=chroma_hostip,\n",
        "    port=chroma_hostport,\n",
        "    headers={'X-Chroma-Token': chroma_api_token}\n",
        ")\n",
        "\n",
        "# @markdown Define your ChromaDB Collection Name - it will create one if it does not exist.\n",
        "vector_collection_name = 'name-of-collection' # @param\n",
        "\n",
        "vectorstore = Chroma(\n",
        "    client=client,\n",
        "    collection_name=vector_collection_name,\n",
        "    embedding_function=embeddings,\n",
        ")\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "# Chaining all input functions together\n",
        "_inputs = RunnableMap(\n",
        "    standalone_question=RunnablePassthrough.assign(\n",
        "        chat_history=lambda x: _format_chat_history(x[\"chat_history\"])\n",
        "    )\n",
        "    | CONDENSE_QUESTION_PROMPT\n",
        "    | ChatOpenAI(temperature=0)\n",
        "    | StrOutputParser(),\n",
        ")\n",
        "_context = {\n",
        "    \"context\": itemgetter(\"standalone_question\") | retriever | _combine_documents,\n",
        "    \"question\": lambda x: x[\"standalone_question\"],\n",
        "}\n",
        "\n",
        "\n",
        "# User input store\n",
        "class ChatHistory(BaseModel):\n",
        "    \"\"\"Chat history with the bot.\"\"\"\n",
        "\n",
        "    chat_history: List[Tuple[str, str]] = Field(\n",
        "        ...,\n",
        "        extra={\"widget\": {\"type\": \"chat\", \"input\": \"question\"}},\n",
        "    )\n",
        "    question: str\n",
        "\n",
        "# Define LLM to use for chat output\n",
        "llm = ChatOpenAI() # Using OpenAI\n",
        "\n",
        "# Putting the conversation chain together\n",
        "conversational_qa_chain = (\n",
        "    _inputs | _context | ANSWER_PROMPT | llm | StrOutputParser()\n",
        ")\n",
        "chain = conversational_qa_chain.with_types(input_type=ChatHistory)\n",
        "\n",
        "# Defining FastAPI app server\n",
        "app = FastAPI(\n",
        "    title=\"LangChain Server\",\n",
        "    version=\"1.0\",\n",
        "    description=\"Spin up a simple api server using Langchain's Runnable interfaces\",\n",
        ")\n",
        "\n",
        "# Adds FastAPI CORS to allow for applications to request externally\n",
        "origins = [\n",
        "    \"*\"\n",
        "]\n",
        "\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=origins,\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "# Adds FastAPI routes to the app for using the chain under:\n",
        "# /invoke\n",
        "# /batch\n",
        "# /stream\n",
        "\n",
        "add_routes(app, chain, enable_feedback_endpoint=True)\n",
        "\n",
        "# @markdown Create remote runnable to allow for use in other langchains (Optional)\n",
        "remote_runnable_host = \"http://0.0.0.0:8000/\" # @param {type:\"string\"}\n",
        "remote_runnable = RemoteRunnable(remote_runnable_host)\n",
        "\n",
        "# Start Server\n",
        "if __name__ == \"__main__\":\n",
        "    import uvicorn\n",
        "\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
        "\n",
        "# Adding Mangum Handler for AWS Lambda\n",
        "handler = Mangum(app)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown ###Option 2 - Open Source\n",
        "\n",
        "from operator import itemgetter\n",
        "from typing import List, Tuple\n",
        "import chromadb\n",
        "import os\n",
        "import langserve\n",
        "import mangum\n",
        "\n",
        "from fastapi import FastAPI\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.embeddings import HuggingFaceHubEmbeddings\n",
        "from langchain.llms import HuggingFaceEndpoint\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.prompts.prompt import PromptTemplate\n",
        "from langchain.schema import format_document\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain.schema.runnable import RunnableMap, RunnablePassthrough\n",
        "from langchain.vectorstores import Chroma\n",
        "from langserve import RemoteRunnable\n",
        "from mangum import Mangum\n",
        "\n",
        "from langserve import add_routes\n",
        "from langserve.pydantic_v1 import BaseModel, Field\n",
        "\n",
        "# @markdown Enter your API Keys\n",
        "\n",
        "# @markdown When using HuggingFace Hub make sure to use the WRITE API Token\n",
        "hugging_face_key = \"hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\" # @param {type:\"string\"}\n",
        "\n",
        "# @markdown Unset the below if you don't want to use OpenAI/ChatOpenAI (currently Llama2 Chat is set behind an approval process) - ChatOpenAI is the default.\n",
        "openai_key = \"sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\" # @param {type:\"string\"}\n",
        "\n",
        "# Set env API Key\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = hugging_face_key\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai_key\n",
        "\n",
        "# Create a template where chat history provides additional dynamic context for next answer\n",
        "_TEMPLATE = \"\"\"Given the following conversation and a follow up question, rephrase the\n",
        "follow up question to be a standalone question, in its original language.\n",
        "\n",
        "Chat History:\n",
        "{chat_history}\n",
        "Follow Up Input: {question}\n",
        "Standalone question:\"\"\"\n",
        "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_TEMPLATE)\n",
        "\n",
        "# Create a standard Q&A template to provide additional persistent context for the next answer\n",
        "ANSWER_TEMPLATE = \"\"\"You are a world-class baker and pastry chef.\n",
        "I will ask you a question and you will provide an answer based on the most relevant recipe you know, as well as commentary on best practices, tips, and any related baked goods that should be considered.\n",
        "\n",
        "You will follow ALL the rules below:\n",
        "\n",
        "1. You will use recipes that are in the database.\n",
        "2. If you cannot find a relevant recipe in the database you will come up with a relevant receipe in the same style as the recipes in the database.\n",
        "\n",
        "Here is the most relevant recipe in the database:\n",
        "{context}\n",
        "\n",
        "This is the question you are being asked: {question}\n",
        "\n",
        "Please answer with the above context.\n",
        "\"\"\"\n",
        "ANSWER_PROMPT = ChatPromptTemplate.from_template(ANSWER_TEMPLATE)\n",
        "\n",
        "DEFAULT_DOCUMENT_PROMPT = PromptTemplate.from_template(template=\"{page_content}\")\n",
        "\n",
        "# Formatting functions\n",
        "def _combine_documents(\n",
        "    docs, document_prompt=DEFAULT_DOCUMENT_PROMPT, document_separator=\"\\n\\n\"\n",
        "):\n",
        "    \"\"\"Combine documents into a single string.\"\"\"\n",
        "    doc_strings = [format_document(doc, document_prompt) for doc in docs]\n",
        "    return document_separator.join(doc_strings)\n",
        "\n",
        "def _format_chat_history(chat_history: List[Tuple]) -> str:\n",
        "    \"\"\"Format chat history into a string.\"\"\"\n",
        "    buffer = \"\"\n",
        "    for dialogue_turn in chat_history:\n",
        "        human = \"Human: \" + dialogue_turn[0]\n",
        "        ai = \"Assistant: \" + dialogue_turn[1]\n",
        "        buffer += \"\\n\" + \"\\n\".join([human, ai])\n",
        "    return buffer\n",
        "\n",
        "# Defining Embedding Function\n",
        "model = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "embeddings = HuggingFaceHubEmbeddings(\n",
        "                model=model,\n",
        "            )\n",
        "\n",
        "# @markdown Define your ChromaDB client/server settings\n",
        "chroma_hostip = '0.0.0.0' # @param {type:\"string\"}\n",
        "chroma_hostport = '8000' # @param {type:\"string\"}\n",
        "chroma_api_token = '0123456789' # @param {type:\"string\"}\n",
        "\n",
        "client = chromadb.HttpClient(\n",
        "    host=chroma_hostip,\n",
        "    port=chroma_hostport,\n",
        "    headers={'X-Chroma-Token': chroma_api_token}\n",
        ")\n",
        "\n",
        "# @markdown Define your ChromaDB Collection Name - it will create one if it does not exist.\n",
        "vector_collection_name = 'name-of-collection' # @param {type:\"string\"}\n",
        "\n",
        "vectorstore = Chroma(\n",
        "    client=client,\n",
        "    collection_name=vector_collection_name,\n",
        "    embedding_function=embeddings,\n",
        ")\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "# @markdown Define LLM to use for chat and parameters - in order to retain the chat history information\n",
        "# @markdown it is advised to choose an open source LLM that provides 16k or 32k context token allowance - keep in mind this will require a large amount of memory in your GPU instance. For example: NurtureAI/OpenHermes-2.5-Mistral-7B-16k\n",
        "endpoint_url = \"(\\\"https://endpoint-url.us-east-1.aws.endpoints.huggingface.cloud\\\")\" # @param {type:\"string\"}\n",
        "llm = HuggingFaceEndpoint(\n",
        "    endpoint_url=endpoint_url,\n",
        "    huggingfacehub_api_token=hugging_face_key,\n",
        "    task='text-generation'\n",
        ")\n",
        "\n",
        "# Chaining all input functions together\n",
        "_inputs = RunnableMap(\n",
        "    standalone_question=RunnablePassthrough.assign(\n",
        "        chat_history=lambda x: _format_chat_history(x[\"chat_history\"])\n",
        "    )\n",
        "    | CONDENSE_QUESTION_PROMPT\n",
        "    | llm\n",
        "    | StrOutputParser(),\n",
        ")\n",
        "_context = {\n",
        "    \"context\": itemgetter(\"standalone_question\") | retriever | _combine_documents,\n",
        "    \"question\": lambda x: x[\"standalone_question\"],\n",
        "}\n",
        "\n",
        "\n",
        "# User input store\n",
        "class ChatHistory(BaseModel):\n",
        "    \"\"\"Chat history with the bot.\"\"\"\n",
        "\n",
        "    chat_history: List[Tuple[str, str]] = Field(\n",
        "        ...,\n",
        "        extra={\"widget\": {\"type\": \"chat\", \"input\": \"question\"}},\n",
        "    )\n",
        "    question: str\n",
        "\n",
        "# Putting the conversation chain together\n",
        "conversational_qa_chain = (\n",
        "    _inputs | _context | ANSWER_PROMPT | llm | StrOutputParser()\n",
        ")\n",
        "chain = conversational_qa_chain.with_types(input_type=ChatHistory)\n",
        "\n",
        "# Defining FastAPI app server\n",
        "app = FastAPI(\n",
        "    title=\"LangChain Server\",\n",
        "    version=\"1.0\",\n",
        "    description=\"Spin up a simple api server using Langchain's Runnable interfaces\",\n",
        ")\n",
        "\n",
        "# Adds FastAPI CORS to allow for applications to request externally\n",
        "origins = [\n",
        "    \"*\"\n",
        "]\n",
        "\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=origins,\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "# Adds FastAPI routes to the app for using the chain under:\n",
        "# /invoke\n",
        "# /batch\n",
        "# /stream\n",
        "\n",
        "add_routes(app, chain, enable_feedback_endpoint=True)\n",
        "\n",
        "# @markdown Create remote runnable to allow for use in other langchains\n",
        "remote_runnable_host = \"http://0.0.0.0:8000/\" # @param\n",
        "remote_runnable = RemoteRunnable(remote_runnable_host)\n",
        "\n",
        "# Start Server\n",
        "if __name__ == \"__main__\":\n",
        "    import uvicorn\n",
        "\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
        "\n",
        "# Adding Mangum Handler for AWS Lambda\n",
        "handler = Mangum(app)"
      ],
      "metadata": {
        "id": "V6Olv4enwfAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0Hfa6m1TznU"
      },
      "source": [
        "Execute the `main.py` file directly and it should start the local LangServe FastAPI server.\n",
        "\n",
        "Test the server is returning an output:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_n8fzIhbTxxx"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "inputs = {\"input\": {\"question\": \"What happens if you don't have high-quality ingredients?\", \"chat_history\": []}}\n",
        "\n",
        "# @markdown Input host server IP and port\n",
        "host_server = \"http://localhost:8000\" # @param\n",
        "\n",
        "response = requests.post(host_server+\"/invoke\", json=inputs)\n",
        "\n",
        "response.json()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTXEFtCKV7Yp"
      },
      "source": [
        "You can also test by going to the `/playground/` page of the host server (default is `localhost:8000/playground`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14NliYflmhKO"
      },
      "source": [
        "# Deploy LangServe to AWS and connect your domain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmHG5GollryV"
      },
      "source": [
        "To ready LangServe for deployment let's package it into a Docker container.\n",
        "\n",
        "This follows the FastAPI documentation for Docker deployment: https://fastapi.tiangolo.com/deployment/docker/\n",
        "\n",
        "Docker is required for this step: https://docs.docker.com/get-docker/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5LwgglIkHD0"
      },
      "source": [
        "---\n",
        "\n",
        "**Create the Custom LangServe Docker Image to be deployed**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTZSLs6_kQRz"
      },
      "source": [
        "Create a `requirements.txt` file in the same directory as your `main.py` project folder.\n",
        "\n",
        "Paste the following code into it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_eY_uuZmY-2"
      },
      "outputs": [],
      "source": [
        "chromadb-client==0.4.19.dev0\n",
        "fastapi==0.108.0\n",
        "langchain==0.0.353\n",
        "langchain_core==0.1.4\n",
        "langserve==0.0.37\n",
        "mangum==0.17.0\n",
        "uvicorn==0.25.0\n",
        "openai==1.6.1\n",
        "sse_starlette==1.8.2\n",
        "huggingface_hub==0.19.4\n",
        "opentelemetry-exporter-otlp-proto-grpc==1.22.0\n",
        "opentelemetry-sdk==1.22.0\n",
        "opentelemetry-api==1.22.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9PkrH31mz1f"
      },
      "source": [
        "Create a `Dockerfile` file in the same directory with the following instructions within:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hxTp0WXLkmqC"
      },
      "outputs": [],
      "source": [
        "# Pull from official python image as base\n",
        "FROM python:3.11\n",
        "\n",
        "# Set working dir\n",
        "WORKDIR /code\n",
        "\n",
        "# Copy python reqs to working dir\n",
        "COPY ./requirements.txt /code/requirements.txt\n",
        "\n",
        "# Install python reqs\n",
        "RUN pip install --no-cache-dir --upgrade pip && \\\n",
        "    pip install --no-cache-dir --upgrade -r /code/requirements.txt\n",
        "\n",
        "# Copy python code to working dir\n",
        "COPY ./app /code/app\n",
        "\n",
        "# start the server\n",
        "CMD [\"uvicorn\", \"app.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ACKzjc4nCYd"
      },
      "source": [
        "Create a `/app/` folder and move the `main.py` file into it. Create a blank `__init__.py` file inside the `/app/` folder as well.\n",
        "\n",
        "The folder structure within the project folder should look something like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JqrodpX9nwGw"
      },
      "outputs": [],
      "source": [
        "./\n",
        "â”œâ”€â”€ Dockerfile\n",
        "â”œâ”€â”€ app\n",
        "â”‚    â”œâ”€â”€ __init__.py\n",
        "â”‚    â””â”€â”€ main.py\n",
        "â””â”€â”€ requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QP0TPx7pEis6"
      },
      "source": [
        "Finally you will need to make some changes to the `main.py` file as we will be starting LangServe using Docker `CMD` instead of within the Python Script itself.\n",
        "\n",
        "Comment out the following code blocks (we're also commenting out `remote_runnable_host` as we're not going to be adding this to any additional Langchains once deployed):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1SbMgWsyEvAA",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @markdown [OPTIONAL] Create remote runnable to allow for use in other langchains\n",
        "# remote_runnable_host = \"http://0.0.0.0:8000/\" # @param\n",
        "# remote_runnable = RemoteRunnable(remote_runnable_host)\n",
        "\n",
        "# Start Server\n",
        "# if __name__ == \"__main__\":\n",
        "#     import uvicorn\n",
        "\n",
        "#     uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXpkgcGsnO30"
      },
      "source": [
        "Ensure that you are in the root of the project folder where the `Dockerfile` is located. You can check by running `ls`. Build your Docker image by running the following command (replace `name-of-image` with a name of your choosing)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XzvTEa1sllgI"
      },
      "outputs": [],
      "source": [
        "docker build -t name-of-image ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3l55JR3WBjyD"
      },
      "source": [
        "If you are running an M[1-3] Mac using an ARM CPU, you will *need* to specify a build for `Linux amd64` architecture otherwise it will default to `arm64` and will not work on `amd64` architecture. This is going to need the use of Docker's experimental Buildkit `buildx` commands.\n",
        "\n",
        "Start by going to your Docker Desktop Dashboard, click the gear icon in the top right corner to access settings and select \"Docker Engine\". You will need to modify your settings to look something like this (Apply & Reset):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9MpLRr-GCKXv"
      },
      "outputs": [],
      "source": [
        "{\n",
        "  \"builder\": {\n",
        "    \"gc\": {\n",
        "      \"defaultKeepStorage\": \"20GB\",\n",
        "      \"enabled\": true\n",
        "    }\n",
        "  },\n",
        "  \"experimental\": true,\n",
        "  \"features\": {\n",
        "    \"buildkit\": true\n",
        "  }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCXvPsc0HEiN"
      },
      "source": [
        "To build for `amd64` architecture you're going to need to use the `buildx` commands and push directly to ECR from Docker Hub (this takes a *long* time).\n",
        "\n",
        "Create your ECR Repo first (refer below) and replace the `[AWS-account-number], [repo-region], [Repository-name]` variables without the square brackets in the code snippet below.\n",
        "\n",
        "Again, reminder, you will need to be in the same project folder where your `Dockerfile` exists:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ASbtPmXmHBP-"
      },
      "outputs": [],
      "source": [
        "docker buildx create --use \\\n",
        "docker buildx build --platform linux/amd64,linux/arm64 --push -t [AWS-account-number].dkr.ecr.[repo-region].amazonaws.com/[Repository-name] ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9teHET7Tly44"
      },
      "source": [
        "Once done, you can test the Docker image is running correctly by running it with this command below (note that the container is mapped to `port 8000`). You can check within your browser by checking the address printed in the output and going to the `/docs/` page where the FastAPI swagger will be shown (where `0.0.0.0` is the default - this will be a different public IP once deployed):\n",
        "\n",
        "`INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxYDuDaalzNd"
      },
      "outputs": [],
      "source": [
        "docker run -p 8000:8000 name-of-image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63NxEojpsu4x"
      },
      "source": [
        "---\n",
        "\n",
        "**Setting Up AWS Credentials and push Docker Image to AWS ECR**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNqGfndXS26U"
      },
      "source": [
        "From here you can deploy the Docker image to the container host of your choice. To keep the image secure I recommend a private image repo such as AWS ECR (https://aws.amazon.com/ecr/).\n",
        "\n",
        "To do so, you will need to ensure you have AWS CLI installed before beginning: https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html\n",
        "\n",
        "AWS Documentation: https://docs.aws.amazon.com/AmazonECR/latest/userguide/docker-push-ecr-image.html\n",
        "\n",
        "You can then configure AWS CLI to easily log in by using (to get your Access Key and Secret Key *you will need to create an IAM User Group and User*).\n",
        "\n",
        "AWS Documentation: https://docs.aws.amazon.com/cli/latest/userguide/cli-authentication-user.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KbZtyuCHnPB7"
      },
      "outputs": [],
      "source": [
        "aws configure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_QgXK8ynN9x"
      },
      "source": [
        "If you don't remember your credentials from above or have misplaced them you can find them in `~/.aws/credentials` on Linux and macOS systems, and `%USERPROFILE%\\.aws\\credentials` on Windows.\n",
        "\n",
        "If you want to create new ones type this into your terminal (and replace `[IAM-user]` with your IAM username - string expected):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7XekA88Fqsqg"
      },
      "outputs": [],
      "source": [
        "aws iam create-access-key [IAM-user]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bV_sUSdoqahR"
      },
      "source": [
        "The easiest way to push to AWS ECR, is create a repo and within the created repo will be a button to \"View push commands\".\n",
        "\n",
        "There should be 4 commands. Copy and paste these commands in the terminal while you are still located in the project folder which contains the `Dockerfile`.\n",
        "\n",
        "This will log you into AWS via the command line with a retrieved token, rebuild the Docker image (make sure you have commented out the blocks of code above), tag it to your created ECR repo and push the image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYsBcUGm0kJF"
      },
      "source": [
        "---\n",
        "\n",
        "**Deploying to EC2**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNOStk8F0nMY"
      },
      "source": [
        "***Pre-requisites: Do you will a Domain Name, this demo does not go into registering nor managing a Domain Name. However, it is recommended that you purchase, register and manage your Domain Name using Cloudflare: https://developers.cloudflare.com/registrar/get-started/register-domain/***\n",
        "\n",
        "---\n",
        "\n",
        "Create your AWS CloudFormation (https://aws.amazon.com/cloudformation/) stack for your LangServe FastAPI Server on EC2 with the following YAML template (copy, paste and save as a YAML file then use the \"Upload a template file\" option when creating a new stack).\n",
        "\n",
        "The choice of EC2 is made for this demo to persist the `/playground/` chat UI.\n",
        "\n",
        "All parameters are *mandatory and must be filled out.*\n",
        "\n",
        "*NOTE: This template assumes the `AWSRegion` resource is the same for both the EC2 instance AND the ECR Repo and that you will set your `VpcId` resource to be the same as the one set for the ChromaDB Vector DB already created above (you can create a new one prior if you wish)*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This YAML script automates the setup and deployment of a the Dockerized LangServe FastAPI server on an EC2 instance. It updates system packages, installs required dependencies, configuring AWS CLI, pulls and runs your LangServe Docker image from ECR, sets up nginx as a reverse proxy to the container with an auto-renewing SSL certificate for a custom domain [YOUR_DOMAIN_NAME], and pulls multiple Github Gists and a cron job to ensure the Docker container and SSL certificate stay updated after deployment.\n",
        "\n",
        "**Please enter your custom domain name below without the HTTPS/HTTP prefix e.g. `api.robs.kitchen`**"
      ],
      "metadata": {
        "id": "NGF94qTgpLNj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mfcqknJJ1jWF"
      },
      "outputs": [],
      "source": [
        "AWSTemplateFormatVersion: '2010-09-09'\n",
        "Description: Creates an EC2 instance with custom security group and docker container\n",
        "\n",
        "Parameters:\n",
        "  InstanceType:\n",
        "    Type: String\n",
        "    AllowedValues:\n",
        "       - c5.xlarge\n",
        "       - c5.2xlarge\n",
        "       - c5.4xlarge\n",
        "       - c5.9xlarge\n",
        "       - c5.18xlarge\n",
        "       - m5.xlarge\n",
        "       - m5.2xlarge\n",
        "       - m5.4xlarge\n",
        "       - m5.12xlarge\n",
        "       - m5.24xlarge\n",
        "       - t2.nano\n",
        "       - t2.micro\n",
        "       - t2.small\n",
        "       - t2.medium\n",
        "       - t2.large\n",
        "       - t2.xlarge\n",
        "       - t2.2xlarge\n",
        "       - t3.nano\n",
        "       - t3.micro\n",
        "       - t3.small\n",
        "       - t3.medium\n",
        "       - t3.large\n",
        "       - t3.xlarge\n",
        "       - t3.2xlarge\n",
        "    Description: Amazon EC2 instance type\n",
        "\n",
        "  AWSRegion:\n",
        "    Description: AWS region where resources will be created\n",
        "    Type: String\n",
        "    AllowedValues:\n",
        "      - ap-southeast-2\n",
        "      - ap-southeast-1\n",
        "      - ap-south-1\n",
        "      - eu-north-1\n",
        "      - eu-west-3\n",
        "      - eu-west-2\n",
        "      - eu-west-1\n",
        "      - ap-northeast-3\n",
        "      - ap-northeast-2\n",
        "      - ap-northeast-1\n",
        "      - ca-central-1\n",
        "      - sa-east-1\n",
        "      - eu-central-1\n",
        "      - us-east-1\n",
        "      - us-east-2\n",
        "      - us-west-1\n",
        "      - us-west-2\n",
        "\n",
        "  EcrImageUri:\n",
        "    Type: String\n",
        "    Description: ECR Image URI for the docker container\n",
        "\n",
        "  VpcId:\n",
        "    Type: AWS::EC2::VPC::Id\n",
        "    Description: VPC Id for the security group\n",
        "\n",
        "  IAMAccessKey:\n",
        "    Type: String\n",
        "    Description: IAM Access Key to authenticate ECR Pull\n",
        "  IAMSecretKey:\n",
        "    Type: String\n",
        "    Description: IAM Secret Key to authenticate ECR Pull\n",
        "\n",
        "Resources:\n",
        "\n",
        "  EC2ContainerServiceRole:\n",
        "    Type: AWS::IAM::Role\n",
        "    Properties:\n",
        "      ManagedPolicyArns:\n",
        "        - arn:aws:iam::aws:policy/service-role/AmazonEC2ContainerServiceforEC2Role\n",
        "      AssumeRolePolicyDocument:\n",
        "        Statement:\n",
        "          - Effect: Allow\n",
        "            Principal:\n",
        "              Service: [ec2.amazonaws.com]\n",
        "            Action: ['sts:AssumeRole']\n",
        "      Policies:\n",
        "        - PolicyName: ecr-permissions\n",
        "          PolicyDocument:\n",
        "            Version: \"2012-10-17\"\n",
        "            Statement:\n",
        "              - Effect: Allow\n",
        "                Action:\n",
        "                  - \"ecr:BatchGetImage\"\n",
        "                  - \"ecr:GetDownloadUrlForLayer\"\n",
        "                  - \"ecr:GetAuthorizationToken\"\n",
        "                Resource: \"*\"\n",
        "\n",
        "  EC2InstanceProfile:\n",
        "    Type: AWS::IAM::InstanceProfile\n",
        "    Properties:\n",
        "      Roles:\n",
        "        - !Ref EC2ContainerServiceRole\n",
        "\n",
        "  EC2Instance:\n",
        "    Type: 'AWS::EC2::Instance'\n",
        "    Properties:\n",
        "      ImageId: !FindInMap [Region2AMI, !Ref 'AWSRegion', AMI]\n",
        "      InstanceType: !Ref InstanceType\n",
        "      IamInstanceProfile: !Ref EC2InstanceProfile\n",
        "      SecurityGroups:\n",
        "        - !Ref InstanceSecurityGroup\n",
        "      UserData:\n",
        "        Fn::Base64: !Sub |\n",
        "          #!/bin/bash\n",
        "          sudo yum check-update\n",
        "          sudo yum update -y\n",
        "          sudo amazon-linux-extras install docker epel -y\n",
        "          sudo amazon-linux-extras enable docker\n",
        "          sudo yum-config-manager --enable epel\n",
        "          sudo yum install amazon-ecr-credential-helper git nginx openssl certbot python-certbot-nginx -y\n",
        "          sudo service docker start\n",
        "          aws configure set output json\n",
        "          aws configure set region ${AWSRegion}\n",
        "          aws configure set aws_access_key_id ${IAMAccessKey}\n",
        "          aws configure set aws_secret_access_key ${IAMSecretKey}\n",
        "          aws ecr get-login-password --region ${AWSRegion} | sudo docker login --username AWS --password-stdin $(echo ${EcrImageUri} | cut -d'/' -f1)\n",
        "          sudo docker pull ${EcrImageUri}\n",
        "          sudo docker run -dit --restart unless-stopped -p 8000:8000 ${EcrImageUri}\n",
        "          sudo git clone https://github.com/dr-robert-li/docker_boot.git /home/ec2-user/docker_boot/\n",
        "          sudo chmod +x /home/ec2-user/docker_boot/run.sh\n",
        "          sudo cp -v /home/ec2-user/docker_boot/docker_boot.service /etc/systemd/system\n",
        "          sudo systemctl enable docker_boot.service\n",
        "          sudo systemctl status docker_boot.service\n",
        "          export PUBLICIP=$(TOKEN=$(curl -sX PUT \"http://169.254.169.254/latest/api/token\" -H \"X-aws-ec2-metadata-token-ttl-seconds: 21600\"); curl -s -H \"X-aws-ec2-metadata-token: $TOKEN\" http://169.254.169.254/latest/meta-data/public-ipv4;)\n",
        "          export YOUR_DOMAIN_NAME=\"api.example.com\" # @param {type:\"string\"}\n",
        "          git clone https://gist.github.com/dr-robert-li/f53a0fc6f6a5e4a961b849f15bd5c0f0 /etc/nginx/templates.d\n",
        "          sudo mv /etc/nginx/nginx.conf /etc/nginx/nginx.conf.original\n",
        "          envsubst < /etc/nginx/templates.d/nginx.conf.template > /etc/nginx/nginx.conf\n",
        "          sudo certbot --nginx -d ${YOUR_DOMAIN_NAME} --register-unsafely-without-email --agree-tos\n",
        "          sudo service nginx start\n",
        "          sudo service nginx reload\n",
        "          (crontab -l ; echo \"00 03 * * * certbot renew --agree-tos\") | crontab -\n",
        "  InstanceSecurityGroup:\n",
        "    Type: 'AWS::EC2::SecurityGroup'\n",
        "    Properties:\n",
        "      GroupDescription: Enable SSH, HTTP and HTTPS access on ports 22, 8000, 80 and 443\n",
        "      SecurityGroupIngress:\n",
        "        - CidrIp: 0.0.0.0/0\n",
        "          FromPort: 8000\n",
        "          ToPort: 8000\n",
        "          IpProtocol: tcp\n",
        "        - CidrIpv6: ::/0\n",
        "          FromPort: 8000\n",
        "          ToPort: 8000\n",
        "          IpProtocol: tcp\n",
        "        - CidrIp: 0.0.0.0/0\n",
        "          FromPort: 443\n",
        "          ToPort: 443\n",
        "          IpProtocol: tcp\n",
        "        - CidrIpv6: ::/0\n",
        "          FromPort: 443\n",
        "          ToPort: 443\n",
        "          IpProtocol: tcp\n",
        "        - CidrIp: 0.0.0.0/0\n",
        "          FromPort: 22\n",
        "          ToPort: 22\n",
        "          IpProtocol: tcp\n",
        "      SecurityGroupEgress:\n",
        "        - CidrIp: 0.0.0.0/0\n",
        "          FromPort: 443\n",
        "          ToPort: 443\n",
        "          IpProtocol: tcp\n",
        "        - CidrIp: 0.0.0.0/0\n",
        "          FromPort: 80\n",
        "          ToPort: 80\n",
        "          IpProtocol: tcp\n",
        "        - CidrIpv6: ::/0\n",
        "          FromPort: 80\n",
        "          ToPort: 80\n",
        "          IpProtocol: tcp\n",
        "\n",
        "Mappings:\n",
        "  Region2AMI:\n",
        "    ap-south-1:\n",
        "      AMI: ami-03cb1380eec7cc118\n",
        "    eu-north-1:\n",
        "      AMI: ami-078e13ebe3b027f1c\n",
        "    eu-west-3:\n",
        "      AMI: ami-00575c0cbc20caf50\n",
        "    eu-west-2:\n",
        "      AMI: ami-0b026d11830afcbac\n",
        "    eu-west-1:\n",
        "      AMI: ami-06e0ce9d3339cb039\n",
        "    ap-northeast-3:\n",
        "      AMI: ami-0171e161a6e0c595c\n",
        "    ap-northeast-2:\n",
        "      AMI: ami-0eb14fe5735c13eb5\n",
        "    ap-northeast-1:\n",
        "      AMI: ami-08a8688fb7eacb171\n",
        "    ca-central-1:\n",
        "      AMI: ami-0843f7c45354d48b5\n",
        "    sa-east-1:\n",
        "      AMI: ami-0344e5787e2e93144\n",
        "    ap-southeast-1:\n",
        "      AMI: ami-0753e0e42b20e96e3\n",
        "    ap-southeast-2:\n",
        "      AMI: ami-047dcdc46ac4f2e6b\n",
        "    eu-central-1:\n",
        "      AMI: ami-004359656ecac6a95\n",
        "    us-east-1:\n",
        "      AMI: ami-0ed9277fb7eb570c9\n",
        "    us-east-2:\n",
        "      AMI: ami-064ff912f78e3e561\n",
        "    us-west-1:\n",
        "      AMI: ami-0746394790be7162e\n",
        "    us-west-2:\n",
        "      AMI: ami-098e42ae54c764c35\n",
        "\n",
        "Outputs:\n",
        "  InstanceId:\n",
        "    Value: !Ref EC2Instance\n",
        "    Description: Instance Id of newly created EC2 instance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cysbp_NxKzv4"
      },
      "source": [
        "The LangServe server will take a few minutes *after* the instance is stood up and passes checks. To see where it's up to connect the instance and run `sudo cat /var/log/cloud-init-output.log`.\n",
        "\n",
        "Just like in the locally hosted version you can check it's working by going to the: `[public-ipv4]:8000/docs` and `[public-ipv4]:8000/playground` pages (make sure to substitute the `[public-ipv4]` variables with the correct Public IPv4 address you can find in the EC2 Instance Summary page. This is if you do NOT have a domain name and will bypass `nginx` altogether (`nginx` will not work in this instance)\n",
        "\n",
        "If you do have a domain name and have applied it correctly then you can access the same at `[YOUR_DOMAIN_NAME]/docs` and `[YOUR_DOMAIN_NAME]/playground`, respectively.\n",
        "\n",
        "For documentation on how to correctly point you A RECORD to the `public-ipv4` address on Cloudflare: https://developers.cloudflare.com/dns/manage-dns-records/how-to/create-dns-records/\n",
        "\n",
        "This will vary depending on your chosen DNS host."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYhlSCJFxzVg"
      },
      "source": [
        "***NOTE: The LangServe service in this demo provides limited resiliency and security measures. In a production setting you may want to apply additional security measures such as blocking unrequired ports including port 8000 and forcing traffic through port 443 using TLS only. You may also want to deploy the an ECR cluster instead, as well for better resiliency***.\n",
        "\n",
        "FastAPI Security Docs: https://fastapi.tiangolo.com/tutorial/security/first-steps/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWSTWefu6kLQ"
      },
      "source": [
        "# Make Production Ready and Integrate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJY_dgQ8T3Om"
      },
      "source": [
        "You're now ready to accept API requests to your LLM!\n",
        "\n",
        "Here is a WordPress plugin that will create a Gutenberg block to interface with the `/invoke/` endpoint of your created API server: https://github.com/dr-robert-li/recipe-bot-langserve-invoke - check the `README.md` file for instructions.\n",
        "\n",
        "Alternatively, you can simply add the URL to the navigation within Appearance options in your Page/Site Editor for a quick and dirty playground.\n",
        "\n",
        "Otherwise API requests will allow you to integrate the Embedded LLM with almost anything, try it out on your `/docs/` Swagger page."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Recommended WordPress Plugins"
      ],
      "metadata": {
        "id": "a4bfXcyh7p8i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://aipower.org/\n",
        "\n",
        "  * AI Toolkit for WordPress that provides SEO and content writing functionality, image generation, fine tuning, embedding and a choice of foundational model.\n",
        "  * More information can be found in the docs: https://docs.aipower.org/docs/category/introduction\n",
        "\n",
        "https://meowapps.com/ai-engine/\n",
        "\n",
        "* Simpler to use AI toolkit that also provides for content generation, embeddings, forms and a chatbot."
      ],
      "metadata": {
        "id": "xZg0gAON6dwp"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "_UfY4NYCOXiL",
        "DK1Upkh3acn7",
        "Dq6Hul2HOGPF",
        "r0vsSzbWQL8X",
        "qzcFnhaFDntN",
        "bBo8sF5E4401",
        "14NliYflmhKO",
        "RWSTWefu6kLQ",
        "a4bfXcyh7p8i"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}